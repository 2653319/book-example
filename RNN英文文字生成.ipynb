{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2653319/book-example/blob/main/RNN%E8%8B%B1%E6%96%87%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
        "\n",
        "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "\n",
        "雖然有些句子是合乎語法的，但大多數都沒有意義。該模型尚未學習單詞的含義，但考慮：\n",
        "\n",
        "* 該模型是基於字符的。訓練開始時，模型不知道如何拼寫一個英文單詞，或者那個單詞甚至是一個文本單元。\n",
        "\n",
        "* 輸出的結構類似於戲劇——文本塊通常以說話者姓名開頭，所有大寫字母與數據集相似。\n",
        "\n",
        "* 如下所示，該模型在小批量文本（每個 100 個字符）上進行訓練，並且仍然能夠生成具有連貫結構的更長文本序列。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "\n",
        "### 下載莎士比亞數據集\n",
        "\n",
        "更改以下行以在您自己的數據上運行此代碼。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811c73f4-6a3f-4793-a572-7f3a4eb20a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### 讀取數據\n",
        "\n",
        "首先，看正文："
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##text"
      ],
      "metadata": {
        "id": "xENyniH60cW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f1cb1c-595b-4117-c352-a7be339dece5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c2212e-b5e3-4589-d1ed-f9f826a2ee75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 查看文本中的前 250 個字符\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e4748d-c3cb-41b3-cc0d-ebc7ab8a1108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# 文件中的唯一字符\n",
        "vocab = sorted(set(text))  #set 將text中 重複的字元刪除  sorted排列\n",
        "print(f'{len(vocab)} unique characters')  #vocab = text裡不重複的全部字元"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 處理文本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 向量化文本\n",
        "\n",
        "在訓練之前，您需要將字符串轉換為數字表示。\n",
        "\n",
        "`tf.keras.layers.StringLookup` 層可以將每個字符轉換為數字 ID。它只需要首先將文本拆分為標記。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2717830-5366-439f-ac6c-612fd6b5d6d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "現在創建 `tf.keras.layers.StringLookup` 層："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(  #根據字典 把字元轉換成標誌\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "\n",
        "它將標記轉換為字符 ID：\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7f65f1-812a-4d19-8e96-c4f523e88d06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)  #由於多一個unk標記  用來標記未出現在字典單字  因此陣列向後增加一ㄍ\n",
        "\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIo7j3NGaNqp",
        "outputId": "12204c10-97e6-4d84-91aa-3f14adc87991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "由於本教程的目標是生成文本，因此反轉此表示並從中恢復人類可讀的字符串也很重要。為此，您可以使用 `tf.keras.layers.StringLookup(..., invert=True)`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "注意：這裡不是傳遞使用 `sorted(set(text))` 生成的原始詞彙表，而是使用 `tf.keras.layers.StringLookup` 層的 `get_vocabulary()` 方法，以便 `[UNK]` 標記是設置方法相同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(    #將標誌轉回字元  vocabulary設定字典 invert反轉開啟\n",
        "  #ids_from_chars.get_vocabulary()取得ids_from_chars字典  因為考慮到UNK標誌問題 所以直接取用  mask_token\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "\n",
        "該層從 ID 的向量中恢復字符，並將它們作為字符的 `tf.RaggedTensor` 返回："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce39499f-fb0f-4bb4-8a32-06a38e3bab41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chars = chars_from_ids(ids) #將標誌轉回字元\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "您可以 `tf.strings.reduce_join` 將字符重新連接成字符串。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9464adb-bf4e-4a53-cbe4-3d2155bfe146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()  #合併"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):  #合併\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "\n",
        "### 預測任務"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "給定一個字符或一個字符序列，最可能的下一個字符是什麼？這是您訓練模型執行的任務。模型的輸入將是一系列字符，您訓練模型以預測輸出——每個時間步的下一個字符。\n",
        "\n",
        "由於 RNN 維持一個依賴於先前看到的元素的內部狀態，給定直到此刻計算的所有字符，下一個字符是什麼？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 創建訓練示例和目標\n",
        "\n",
        "接下來將文本分成示例序列。每個輸入序列都將包含文本中的“seq_length”字符。\n",
        "\n",
        "對於每個輸入序列，對應的目標包含相同長度的文本，除了向右移動一個字符。\n",
        "\n",
        "因此，將文本分解為 `seq_length+1` 的塊。例如，假設 `seq_length` 是 4，我們的文本是“Hello”。輸入序列是“Hell”，目標序列是“ello”。\n",
        "\n",
        "為此，首先使用 `tf.data.Dataset.from_tensor_slices` 函數將文本向量轉換為字符索引流。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.unicode_split(text, 'UTF-8') #一個一個字元 切割text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-O5QXcRdNeo",
        "outputId": "1d6b1f7c-d1be-4e41-ccd3-f527dbd14faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=string, numpy=array([b'F', b'i', b'r', ..., b'g', b'.', b'\\n'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##all_ids  \n",
        "all_ids = 把text切割後轉換成標誌"
      ],
      "metadata": {
        "id": "ZVSfTd9a0XiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed609828-7b52-4fd4-cc03-b27ad1241b1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')) #透過ids_from_chars 將切割後字元 轉換成標誌\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ids_dataset\n",
        "\n",
        "把all_ids 一ㄍ一ㄍ分開"
      ],
      "metadata": {
        "id": "wz7zKskE0Ubl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) #一個一個分開"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.data.Dataset.from_tensor_slices()測試"
      ],
      "metadata": {
        "id": "LzVy5ZfG1BJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])"
      ],
      "metadata": {
        "id": "V8tvZwK9d6TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset:\n",
        "  print(i.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMbMdRBld64Y",
        "outputId": "7487e4d1-360b-4766-b1b5-13afd3a1f9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "3\n",
            "0\n",
            "8\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29d7b90-7a9c-4267-bde8-3618f93b876f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "##sequences\n",
        "將ids_dataset根據seq_length+1 去分批 (每101個一批)\n",
        "\n",
        "`batch` 方法可以讓您輕鬆地將這些單個字符轉換為所需大小的序列。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b09ed63-6de7-4902-da40-8e6bd3a261ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) #依據seq_length去分批id_dataset \n",
        "#drop_remainder  表示在最後一批元素少於元素的情況下是否應刪除;默認行為是不刪除較小的批\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "如果您將標記重新加入字符串，則更容易看到這是在做什麼："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefc2004-e136-4308-9118-d161bc002b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):  #將標誌轉換成文字  然後合併\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "\n",
        "對於訓練，您需要一個“（輸入，標籤）”對的數據集。其中“輸入”和\n",
        "`label` 是序列。在每個時間步，輸入是當前字符，標籤是下一個字符。\n",
        "\n",
        "這是一個函數，它將序列作為輸入，複製並移動它以對齊每個時間步的輸入和標籤："
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##split_input_target\n",
        "將輸入的句子  頭尾各切出 分成2句"
      ],
      "metadata": {
        "id": "5pdK29pl1iwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):  #將句子分成 切出最後一個字和  最前面一個字 兩種\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0dc4da-0e35-4067-88e1-309b3f7d24c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dataset\n",
        "sequences為 分批後句子\n",
        "\n",
        "∇\n",
        "dataset = 把sequences裡(分批後 每批101ㄍ)的全部句子 把頭尾各切掉 分兩句"
      ],
      "metadata": {
        "id": "2WQnwDYB0Clt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)  #map()執行map裡面的函式  把sequences裡(分批後 每批101ㄍ)的全部句子 把頭尾各切掉 分兩句"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103d4fee-b359-4b12-de4e-d4bbf6a366ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Li4ieO3Gwl",
        "outputId": "59287164-9e5c-4ae6-8df7-d9219c92771e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44,\n",
            "       45, 54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40,\n",
            "       53, 64,  2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2,\n",
            "       52, 44,  2, 58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32,\n",
            "       55, 44, 40, 50,  7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57,\n",
            "       58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 38, 54, 60])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
            "       54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53,\n",
            "       64,  2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52,\n",
            "       44,  2, 58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55,\n",
            "       44, 40, 50,  7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58,\n",
            "       59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 38, 54, 60,  2])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 創建訓練批次\n",
        "\n",
        "您使用 `tf.data` 將文本拆分為可管理的序列。但在將這些數據輸入模型之前，您需要將數據打亂並打包成批次。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dataset_2\n",
        "將原本的dataset打亂\n",
        "\n",
        "並再度分批 \n",
        "\n",
        "\n",
        "\n",
        "1.   text : 原始文字檔\n",
        "2.   all_ids : 切割text字元轉換成標記\n",
        "3.   ids_dataset : 將all_ids裡的字元標記 一個一個分開\n",
        "4.   sequences : 將ids_dataset根據seq_length+1 (101) 去分批 (每101個字元標記一批)\n",
        "5.   dataset : 使用函式split_input_target把分批後的(標記)頭跟尾各拆出來 做成兩個句子(有頭無尾 和 有尾無頭 的句子)\n",
        "6.   dataset_2 : 將dataset\n",
        "打亂(內部字元標誌順序)然後在每64個一批(每個皆為100個字元標記 然後每64個一批)\n"
      ],
      "metadata": {
        "id": "tnAjFa-f16yU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1c0566-7e29-49ef-f049-39178fa53446"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 緩衝區大小以打亂數據集\n",
        "# （TF 數據被設計為與可能無限的序列一起工作，\n",
        "# 所以它不會嘗試打亂內存中的整個序列。反而，\n",
        "# 它維護一個緩衝區，在其中對元素進行洗牌）。\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))  #自動優化\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dataset_2 打亂後\n"
      ],
      "metadata": {
        "id": "5297xnNM21U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lrncona2VfF",
        "outputId": "8e56897d-5c99-4f5e-8f97-68575cce33c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(64, 100), dtype=int64, numpy=\n",
            "array([[ 2, 55, 40, ..., 53,  7,  2],\n",
            "       [ 1, 28,  2, ..., 35, 54, 60],\n",
            "       [11,  1, 14, ...,  2, 47, 44],\n",
            "       ...,\n",
            "       [ 2, 62, 48, ..., 48, 59, 47],\n",
            "       [58, 44, 44, ..., 33, 54,  2],\n",
            "       [62,  2, 59, ...,  2, 40,  2]])>, <tf.Tensor: shape=(64, 100), dtype=int64, numpy=\n",
            "array([[55, 40, 64, ...,  7,  2, 14],\n",
            "       [28,  2, 62, ..., 54, 60, 42],\n",
            "       [ 1, 14, 46, ..., 47, 44,  2],\n",
            "       ...,\n",
            "       [62, 48, 59, ..., 59, 47,  2],\n",
            "       [44, 44,  1, ..., 54,  2, 46],\n",
            "       [ 2, 59, 47, ..., 40,  2, 52]])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "本節將模型定義為 `keras.Model` 子類（有關詳細信息，請參閱[通過子類化製作新層和模型](https://www.tensorflow.org/guide/keras/custom_layers_and_models)）。\n",
        "\n",
        "該模型分為三層：\n",
        "\n",
        "* `tf.keras.layers.Embedding`：輸入層。一個可訓練的查找表，它將每個字符 ID 映射到具有“embedding_dim”維度的向量；\n",
        "* `tf.keras.layers.GRU`：一種大小為 `units=rnn_units` 的 RNN（您也可以在此處使用 LSTM 層。）\n",
        "* `tf.keras.layers.Dense`：輸出層，帶有 `vocab_size` 輸出。它為詞彙表中的每個字符輸出一個 logit。這些是根據模型的每個字符的對數似然。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# 詞彙表的長度（以字符為單位）\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # 確保詞彙量大小與“StringLookup”層相匹配。\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "對於每個字符，模型查找嵌入，以嵌入作為輸入運行 GRU 一個時間步長，並應用密集層生成預測下一個字符的對數似然的 logits：\n",
        "\n",
        "![通過模型的數據圖](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "\n",
        "注意：對於訓練，您可以在此處使用 `keras.Sequential` 模型。要稍後生成文本，您需要管理 RNN 的內部狀態。預先包含狀態輸入和輸出選項比稍後重新排列模型架構更簡單。有關詳細信息，請參閱 [Keras RNN 指南](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "##試試模型\n",
        "\n",
        "現在運行模型以查看其行為是否符合預期。\n",
        "\n",
        "首先檢查輸出的形狀："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(input_example_batch[0])\n",
        "  print(target_example_batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee4FqbPmyrxe",
        "outputId": "53bf00f7-5860-4efd-c517-b63fe8745136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[48 53  2 64 54 60 57  2 51 48 55 58  7  1 25 48 50 44  2 52 40 53  2 53\n",
            " 44 62  2 52 40 43 44  9  1  1 14 27 20 18 25 28 11  1 15 44  2 64 54 60\n",
            "  2 42 54 53 59 44 53 59  7  2 45 40 48 57  2 52 40 48 43 12  1 22 59  2\n",
            " 48 58  2 59 47 44  2 51 40 62  7  2 53 54 59  2 22  2 42 54 53 43 44 52\n",
            " 53  2 64 54], shape=(100,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[53  2 64 54 60 57  2 51 48 55 58  7  1 25 48 50 44  2 52 40 53  2 53 44\n",
            " 62  2 52 40 43 44  9  1  1 14 27 20 18 25 28 11  1 15 44  2 64 54 60  2\n",
            " 42 54 53 59 44 53 59  7  2 45 40 48 57  2 52 40 48 43 12  1 22 59  2 48\n",
            " 58  2 59 47 44  2 51 40 62  7  2 53 54 59  2 22  2 42 54 53 43 44 52 53\n",
            "  2 64 54 60], shape=(100,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643e3207-157f-45a9-b1fc-fa15c5a8a5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "在上面的例子中，輸入的序列長度是“100”，但是模型可以在任何長度的輸入上運行："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011facb9-7321-4ecd-9cdd-1199c065d773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "要從模型中獲得實際預測，您需要從輸出分佈中採樣，以獲得實際的字符索引。此分佈由字符詞彙表上的 logits 定義。\n",
        "\n",
        "注意：從這個分佈中 _sample_ 很重要，因為獲取分佈的 _argmax_ 很容易讓模型陷入循環。\n",
        "\n",
        "嘗試批處理中的第一個示例："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDd9sMfJyM8U",
        "outputId": "4c7d3a9a-992e-4c5e-e485-2b1947a3a20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0][0]  #單次  1個"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvjuAlkV84y1",
        "outputId": "62df46b3-83a0-4264-bf42-e0c462567539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(66,), dtype=float32, numpy=\n",
              "array([ 1.47373527e-02, -1.30255602e-03, -3.31095490e-03,  7.74528785e-03,\n",
              "        1.31458358e-03, -3.73415393e-03,  1.16314795e-02,  1.66195945e-03,\n",
              "       -1.79547016e-02, -4.96065523e-03,  5.79529721e-03,  7.91224558e-03,\n",
              "       -3.64950253e-03, -2.98415823e-03, -1.06501579e-03, -1.18054403e-02,\n",
              "       -1.52522810e-02,  1.86658674e-03, -5.11689403e-04, -1.65472403e-02,\n",
              "        3.40337446e-03,  4.06091847e-03, -1.62174031e-02,  1.93740695e-03,\n",
              "        9.90809500e-03, -3.62373376e-03,  1.08096581e-04,  1.04385102e-03,\n",
              "       -4.79410822e-03,  3.93873407e-03, -3.30301351e-03,  1.19510842e-02,\n",
              "        3.03774403e-04,  2.19397270e-03, -8.77941772e-03,  1.95144489e-03,\n",
              "        6.07894408e-03, -1.19723601e-03,  6.87311869e-03, -6.06578030e-03,\n",
              "        3.54992109e-03,  9.24142019e-04, -2.89068383e-04,  5.83804073e-03,\n",
              "       -7.07369717e-03,  5.27616823e-03,  2.30620545e-03, -5.39336959e-03,\n",
              "       -6.15410463e-05,  6.02330547e-03, -7.08821556e-03,  2.53331009e-03,\n",
              "        1.37509257e-02,  7.00566499e-03, -1.00712506e-02, -8.57233722e-03,\n",
              "        6.33928413e-03,  1.64655298e-02, -4.03592084e-03,  9.06199496e-03,\n",
              "       -3.25360917e-03, -8.39027867e-04, -5.94294281e-04,  6.44533243e-03,\n",
              "       -8.67951487e-04, -3.19798780e-03], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0]  #100個字元標記1批 每64批再分成一大批  字典數總共66個 \n",
        "#所以它是預測 每個字元標記(每次一個 總共100個)的下一個標記可能是什麼  所以預測100次 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWaE9elu82pD",
        "outputId": "e7e813f1-16a1-4be7-d74d-fdfa50bd969b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 66), dtype=float32, numpy=\n",
              "array([[ 0.01473735, -0.00130256, -0.00331095, ...,  0.00644533,\n",
              "        -0.00086795, -0.00319799],\n",
              "       [ 0.00506034, -0.00058064,  0.00523378, ...,  0.00789508,\n",
              "        -0.0011092 , -0.002467  ],\n",
              "       [ 0.00409553, -0.00785127,  0.01213993, ...,  0.01036033,\n",
              "         0.00402052,  0.00525858],\n",
              "       ...,\n",
              "       [-0.00615816,  0.00576279,  0.01624059, ...,  0.00175738,\n",
              "         0.00079571, -0.00029083],\n",
              "       [-0.00543165,  0.00352938,  0.01561974, ...,  0.0037165 ,\n",
              "        -0.00096293, -0.00092333],\n",
              "       [-0.00626807,  0.01960578,  0.01079688, ..., -0.00616088,\n",
              "        -0.00654238, -0.00778664]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions  #64大批"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQBG5bvh8wf1",
        "outputId": "9ffd1e0d-a7ca-4ef9-a962-dd3209396676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 100, 66), dtype=float32, numpy=\n",
              "array([[[ 0.01473735, -0.00130256, -0.00331095, ...,  0.00644533,\n",
              "         -0.00086795, -0.00319799],\n",
              "        [ 0.00506034, -0.00058064,  0.00523378, ...,  0.00789508,\n",
              "         -0.0011092 , -0.002467  ],\n",
              "        [ 0.00409553, -0.00785127,  0.01213993, ...,  0.01036033,\n",
              "          0.00402052,  0.00525858],\n",
              "        ...,\n",
              "        [-0.00615816,  0.00576279,  0.01624059, ...,  0.00175738,\n",
              "          0.00079571, -0.00029083],\n",
              "        [-0.00543165,  0.00352938,  0.01561974, ...,  0.0037165 ,\n",
              "         -0.00096293, -0.00092333],\n",
              "        [-0.00626807,  0.01960578,  0.01079688, ..., -0.00616088,\n",
              "         -0.00654238, -0.00778664]],\n",
              "\n",
              "       [[ 0.01473735, -0.00130256, -0.00331095, ...,  0.00644533,\n",
              "         -0.00086795, -0.00319799],\n",
              "        [ 0.00506034, -0.00058064,  0.00523378, ...,  0.00789508,\n",
              "         -0.0011092 , -0.002467  ],\n",
              "        [-0.00055853,  0.01730294,  0.00530882, ..., -0.00309048,\n",
              "         -0.00597432, -0.00824041],\n",
              "        ...,\n",
              "        [-0.00929408,  0.01945426,  0.00994098, ...,  0.00463051,\n",
              "         -0.00647209, -0.00588218],\n",
              "        [-0.00674529,  0.00988195,  0.01342498, ...,  0.00662819,\n",
              "         -0.00294107, -0.00274442],\n",
              "        [ 0.01058451,  0.00493814,  0.0157959 , ...,  0.0072703 ,\n",
              "         -0.0026873 ,  0.00416466]],\n",
              "\n",
              "       [[ 0.01368149,  0.00265987,  0.00851584, ...,  0.00424558,\n",
              "         -0.00227539,  0.00657204],\n",
              "        [-0.00039796,  0.00241773,  0.01544383, ...,  0.00727053,\n",
              "          0.01072991,  0.00334322],\n",
              "        [-0.00389856,  0.01106901,  0.0039432 , ...,  0.01291349,\n",
              "          0.01027583, -0.00113601],\n",
              "        ...,\n",
              "        [ 0.01044057, -0.00794003, -0.00351576, ...,  0.01262346,\n",
              "          0.00927876,  0.00693447],\n",
              "        [ 0.00421436,  0.0105958 ,  0.00899067, ...,  0.00160038,\n",
              "          0.01071799, -0.00608127],\n",
              "        [ 0.00075257,  0.00480217,  0.00116849, ..., -0.00086356,\n",
              "          0.00106694,  0.00035494]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.00174052,  0.01063776, -0.00329402, ...,  0.01002737,\n",
              "          0.00447857, -0.00282864],\n",
              "        [-0.00220036,  0.02124188, -0.0017174 , ...,  0.00039132,\n",
              "          0.00626197, -0.007967  ],\n",
              "        [-0.00805779,  0.02740379, -0.00106689, ..., -0.00586991,\n",
              "         -0.00339226, -0.00642026],\n",
              "        ...,\n",
              "        [-0.00267176,  0.0062291 ,  0.01078095, ...,  0.00675772,\n",
              "         -0.00120434, -0.00270663],\n",
              "        [-0.00397997,  0.02046118,  0.00814876, ..., -0.00331607,\n",
              "         -0.00513095, -0.00793444],\n",
              "        [-0.00411254,  0.02665169,  0.0050253 , ..., -0.00821216,\n",
              "          0.0002152 , -0.01328519]],\n",
              "\n",
              "       [[-0.00381702, -0.00011522,  0.00256851, ...,  0.00089499,\n",
              "          0.00598703,  0.00165857],\n",
              "        [ 0.01180558,  0.00301137,  0.00892543, ...,  0.00345178,\n",
              "          0.00203667,  0.00609313],\n",
              "        [ 0.00722633, -0.00078079,  0.00810996, ...,  0.01132418,\n",
              "          0.0078369 ,  0.0005904 ],\n",
              "        ...,\n",
              "        [-0.00781636,  0.00026199, -0.01451748, ..., -0.00770489,\n",
              "          0.00150888,  0.01501616],\n",
              "        [-0.01852502,  0.0016744 , -0.01647592, ..., -0.01119824,\n",
              "          0.0076749 ,  0.01923929],\n",
              "        [-0.01391611, -0.00313259, -0.0013501 , ..., -0.00192938,\n",
              "          0.0035692 ,  0.01156109]],\n",
              "\n",
              "       [[-0.01266192, -0.00408088, -0.00535043, ...,  0.00399969,\n",
              "         -0.00355025, -0.00373459],\n",
              "        [-0.0031265 , -0.00308964, -0.00167526, ...,  0.01176196,\n",
              "          0.0118149 ,  0.00699866],\n",
              "        [-0.00782703, -0.00153672,  0.00019771, ..., -0.00083773,\n",
              "         -0.00388512, -0.00402782],\n",
              "        ...,\n",
              "        [-0.0139946 , -0.00390575,  0.00515145, ..., -0.0079956 ,\n",
              "         -0.01242637, -0.00949791],\n",
              "        [-0.00879351,  0.0121307 , -0.00471246, ..., -0.00534262,\n",
              "         -0.01524527, -0.00623109],\n",
              "        [ 0.00017717,  0.01101908,  0.00067412, ..., -0.00568001,\n",
              "         -0.01214598, -0.00886638]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3da2d25-c437-4e7c-ded8-81e7c84da99a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 1), dtype=int64, numpy=\n",
              "array([[58],\n",
              "       [38],\n",
              "       [10],\n",
              "       [47],\n",
              "       [44],\n",
              "       [35],\n",
              "       [16],\n",
              "       [10],\n",
              "       [39],\n",
              "       [ 2],\n",
              "       [34],\n",
              "       [33],\n",
              "       [28],\n",
              "       [ 3],\n",
              "       [48],\n",
              "       [50],\n",
              "       [ 6],\n",
              "       [12],\n",
              "       [53],\n",
              "       [20],\n",
              "       [32],\n",
              "       [14],\n",
              "       [57],\n",
              "       [30],\n",
              "       [59],\n",
              "       [20],\n",
              "       [ 9],\n",
              "       [34],\n",
              "       [33],\n",
              "       [ 9],\n",
              "       [21],\n",
              "       [14],\n",
              "       [42],\n",
              "       [50],\n",
              "       [ 8],\n",
              "       [21],\n",
              "       [ 6],\n",
              "       [64],\n",
              "       [58],\n",
              "       [35],\n",
              "       [23],\n",
              "       [36],\n",
              "       [13],\n",
              "       [57],\n",
              "       [58],\n",
              "       [ 5],\n",
              "       [15],\n",
              "       [11],\n",
              "       [34],\n",
              "       [ 8],\n",
              "       [30],\n",
              "       [18],\n",
              "       [26],\n",
              "       [65],\n",
              "       [ 9],\n",
              "       [43],\n",
              "       [ 7],\n",
              "       [48],\n",
              "       [49],\n",
              "       [ 7],\n",
              "       [25],\n",
              "       [35],\n",
              "       [48],\n",
              "       [55],\n",
              "       [ 1],\n",
              "       [48],\n",
              "       [ 8],\n",
              "       [31],\n",
              "       [ 2],\n",
              "       [ 3],\n",
              "       [60],\n",
              "       [43],\n",
              "       [ 9],\n",
              "       [53],\n",
              "       [25],\n",
              "       [43],\n",
              "       [ 6],\n",
              "       [23],\n",
              "       [22],\n",
              "       [26],\n",
              "       [15],\n",
              "       [ 0],\n",
              "       [58],\n",
              "       [19],\n",
              "       [15],\n",
              "       [61],\n",
              "       [34],\n",
              "       [59],\n",
              "       [41],\n",
              "       [17],\n",
              "       [50],\n",
              "       [18],\n",
              "       [62],\n",
              "       [29],\n",
              "       [62],\n",
              "       [43],\n",
              "       [39],\n",
              "       [60],\n",
              "       [17],\n",
              "       [24]])>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) #從分類分佈中抽取樣本。   \n",
        "                          #num_samples 要為每個行切片繪製的獨立樣本數\n",
        "                          #從66個裡面  隨機抽出一個\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() #從張量的形狀中刪除大小為1的維度\n"
      ],
      "metadata": {
        "id": "_Y-8bJ7ayJGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "這給了我們在每個時間步長下一個字符索引的預測："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flIVKZtT-ujU",
        "outputId": "d8d3397f-e488-4612-a0a4-5370a20d3801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 38, 10, 47, 44, 35, 16, 10, 39,  2, 34, 33, 28,  3, 48, 50,  6,\n",
              "       12, 53, 20, 32, 14, 57, 30, 59, 20,  9, 34, 33,  9, 21, 14, 42, 50,\n",
              "        8, 21,  6, 64, 58, 35, 23, 36, 13, 57, 58,  5, 15, 11, 34,  8, 30,\n",
              "       18, 26, 65,  9, 43,  7, 48, 49,  7, 25, 35, 48, 55,  1, 48,  8, 31,\n",
              "        2,  3, 60, 43,  9, 53, 25, 43,  6, 23, 22, 26, 15,  0, 58, 19, 15,\n",
              "       61, 34, 59, 41, 17, 50, 18, 62, 29, 62, 43, 39, 60, 17, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "\n",
        "解碼這些以查看此未經訓練的模型預測的文本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250ef472-28ba-42aa-c1f4-381d2a1b0568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'd souls\\nDo through the clouds behold this present hour,\\nEven for revenge mock my destruction!\\nThis i'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"sY3heVC3Z UTO!ik';nGSArQtG.UT.HAck-H'ysVJW?rs&B:U-QEMz.d,ij,LVip\\ni-R !ud.nLd'JIMB[UNK]sFBvUtbDkEwPwdZuDK\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "\n",
        "此時可以將問題視為標準分類問題。給定之前的 RNN 狀態，以及這個時間步的輸入，預測下一個字符的類別。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "\n",
        "### 附加優化器和損失函數"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "\n",
        "標準的 `tf.keras.losses.sparse_categorical_crossentropy` 損失函數在這種情況下有效，因為它應用於預測的最後一個維度。\n",
        "\n",
        "因為您的模型返回 logits，您需要設置 `from_logits` 標誌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2155b993-6f6e-4bbf-b92d-71a883fee6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1880674, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "\n",
        "一個新初始化的模型不應該對自己太確定，輸出的 logits 應該都有相似的大小。為了確認這一點，您可以檢查平均損失的指數是否大約等於詞彙量。更高的損失意味著模型確定它的錯誤答案，並且初始化錯誤："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ca8e33-4178-41f7-b27d-cf283aada23a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.89532"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "\n",
        "使用 `tf.keras.Model.compile` 方法配置訓練過程。使用帶有默認參數和損失函數的 `tf.keras.optimizers.Adam`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 配置檢查點"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "\n",
        "使用 `tf.keras.callbacks.ModelCheckpoint` 確保在訓練期間保存檢查點："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "\n",
        "### 執行訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "為了保持訓練時間合理，使用 10 個 epoch 來訓練模型。在 Colab 中，將運行時設置為 GPU 以加快訓練速度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c4e712-a8b7-4f12-c1b7-6bf69c93cf8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 28s 140ms/step - loss: 2.7528\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 2.0055\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.7257\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.5621\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.4626\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.3933\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.3405\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.2949\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.2538\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.2149\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "使用此模型生成文本的最簡單方法是循環運行它，並在執行時跟踪模型的內部狀態。\n",
        "\n",
        "![為了生成文本，模型的輸出被反饋到輸入](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "每次調用模型時，都會傳入一些文本和內部狀態。該模型返回對下一個字符及其新狀態的預測。將預測和狀態傳回以繼續生成文本。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "\n",
        "下面進行單步預測："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "在循環中運行它以生成一些文本。查看生成的文本，您會看到該模型知道何時大寫、製作段落並模仿莎士比亞式的寫作詞彙。由於訓練 epoch 的數量很少，它還沒有學會形成連貫的句子。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95939ca-c928-4499-8a47-07f5b57bd14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Were not so can prey their eyes\n",
            "With vishom from himself, some island\n",
            "Were took of all woes, and I'll beg.\n",
            "\n",
            "Bost:\n",
            "Come, sir, I know it so: sir, cut it a tale: speak lagging\n",
            "your virtues, if I do bey dead that breathe it: at\n",
            "Aht on his brother.\n",
            "\n",
            "KING RICHARD II:\n",
            "Nor thy mistress so a noble must was,\n",
            "Thy thrish o' the time ensider than here still, diseace no more\n",
            "of his health; for he is minded, as but\n",
            "our butiness, bad children ere mine enemy.\n",
            "\n",
            "CLIRDEBO:\n",
            "Novergound your fatheritable, that I am done;\n",
            "This is a while he wash'd upon,\n",
            "And no more than a batty offices\n",
            "Than what you wrong'dn me for your honour.\n",
            "Detestom, it so must I see whom comfort you\n",
            "To fear the gaven my state to Beng Surren crast?\n",
            "\n",
            "YORK:\n",
            "Madam, had I know, I woo:' 'tis body, a very strange as their\n",
            "post brought your face gaves have wife for as the stage\n",
            "Unon the right thought on the heart-store.\n",
            "What she was born thine hate;\n",
            "Than sheep you for it. Speak not whither?\n",
            "\n",
            "VOLUMNIA:\n",
            "No sorrow, good days he of mine, some other \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.761098623275757\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "改善結果的最簡單方法是訓練更長時間（嘗試 `EPOCHS = 30`）。\n",
        "\n",
        "您還可以嘗試使用不同的起始字符串，嘗試添加另一個 RNN 層以提高模型的準確性，或者調整溫度參數以生成或多或少的隨機預測。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "\n",
        "如果您希望模型更快地生成文本，那麼您可以做的最簡單的事情就是批量生成文本。在下面的示例中，模型生成 5 個輸出的時間與上面生成 1 個輸出的時間大致相同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595ea8bb-321f-46d4-d14a-7042af196bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWe will not go back.\\n\\nLirYT:\\nI leave? Come.\\n\\nKING RICHARD II:\\nWith think that made Romeo England's business,\\nSo woound alsomen, your outrage to vent,\\nAnd for this country we show morrow might\\nNow York's name awaked alone.\\n\\nHERMIONE:\\nNay, that the trumpets; he would say 'prageno her commission;\\nThe truck is dead assured for gaterful\\nBeholding of all eam but arms.\\n\\nESCALUS:\\nCore, mine, as Evermake him gentleman\\nI' thee, tender but horesty's grave;\\nAnd in a widow, lady! prove a tramber\\nwear them with a service brows, that I slave than heart\\nThat seem'dis--which if you hear?' but not\\nAre damnable, when he shall be irle.\\n\\nLUCENTIO:\\nI shall be committed, my liege!\\n\\nESCALUS:\\nIf we mistank, steal uproy, you shall were wonder-complixing.\\n\\nGLOUCESTER:\\nGentle heaven Saint George of sorrow,\\nWhich, title is a whip day as to-day, the deep wlect your honour!\\nOf more. Hast thou wert stay and speak\\nLest nature on thing enemy, how have lived the thrice doth hard\\nan white, the queen I shall shake I meet\"\n",
            " b\"ROMEO:\\nWe three buriedly.\\n\\nMENENIUS:\\nGo with me?\\n\\nBRUTUS:\\nThe muely sorrow, good cousin,\\nAnd then for Citizens, ties your comply.\\nBook, so no think, thy spies sustormation: the king\\nsuchers of the people; when our petition\\nBy sight on me ignoran, they sigh'd a holy hundring.\\nFirst, then, amen? O straight,\\nWhich, let is bethood in king roke poor queen.\\n\\nDERBY:\\nIt was a man of Lancaster.\\n\\nGLOUCESTER:\\nWhy, sir, therefore, to weep she fraguednaing this angel:\\nCome away, the suberat Proudy, but\\nsteel which now help sleep;\\nAnd thereof the queen's balk down the violy life\\nThat were all the day of Hereford, dast thine eyes, no churchyalted\\nand roused us, and so with me but now releared you?\\n\\nSAMPSON:\\nTrust thereby amblure?\\n\\nThis is your hands\\nThat Londay stopp'd at thy hand Kates.\\nBut sin us all amended it as becomes are.\\n\\nPETRUCHIO:\\nPreas: leave me that list estable our life\\nTragious yearts are tricks of night.\\nLet's strength like a present stamb, we shall not be gone.\\nThese are on it, pray you now\"\n",
            " b\"ROMEO:\\nWhat have and reign is but my prayer under gate.\\nSo first conceive, that is oft sie:\\nBut stay, 'good Paul's your brother are in his sails\\nStand from his haste. Come, lunns more:\\nI have done to ull;\\nAbeach my manly plonation to yet?\\nAs it of sons, growarts, trumpets shall be joyling upon\\nYour voss. Thou some sceet.\\n\\nOXFORD:\\nPlay therein once more, alas, to murder nor ans\\nThat beard it done, be seen,\\nHe fair deliver me, he's remain,\\nAnd banish him that her seek at your sorrow nerd.\\n\\nAUMOLLOUS:\\nWell, do you have; but he shall asswer\\nYour voices so within this body's chose;\\nThe speech of kill despake no choice.\\n\\nSecond Servingman:\\nAnd this be thy minds bening but ut\\ninch of sweets, toosed, from the kill of Warwife:\\nThe brother reason more paper with power:\\nI will bear it.\\n\\nBUCKINGHAM:\\nWas letters you, she--\\nFirst Warwick,\\nWere he nothing that thou camest thine ear;\\nI would they merry entrailly to your fools,\\nYoung minder-like, begin?\\n\\nBUCKINGHAM:\\nI speak; sir; were I know,\\nThree darest to\"\n",
            " b\"ROMEO:\\nWhat, go with us alike.\\n\\nPedant:\\nTheir; was the grace a way to ply down:\\nThe nexts and shake is in every ones of your soul,\\nBut is it cleared to puison this man to bed common both\\nOur streets that have him will it.\\n\\nKING RICHARD III:\\nI know you royal pilot\\nThat thine own inle in earth and spend it\\nThan a wise agien.\\n\\nWould:\\nWell, well we here?\\nTo Great kind and young he had haste; and bear\\nA beauty of the tild went them know.\\n\\nANTONIO:\\nTwenty troops I tender men he'll fear\\nTo suke to entertain than die.\\n\\nLUCIO:\\nNo, my good Pompey, twenty arms:\\nIf I must perjury, by your\\nsome curse to be your son, his princely buzzed thy hence.\\n\\nBENHOMIO:\\nWouchful feeling of fooriest kind it.\\n\\nDUKE OF AUMERLE:\\nWhy, that's the fatish? Taking the walf; bring yied\\nTo give the trust early-dangering with\\nFather than he had our injurious bill.\\n\\nTESSLY:\\nMy lord, I say.\\n\\nLORD WeSREY:\\nNo soul the quaint that it pass? it is so blood?\\n\\nLays Murder:\\nTo Aundrike, dead and in thy death,\\nAnd speak the fairest than aw\"\n",
            " b\"ROMEO:\\nThen, as you would love thee, go.\\n\\nDUCHESS\\nOF YORK:\\nNo sheek hath some vipoo a kind;\\nAnd I, my exilence. Ty and the promods\\nIs mine abears, Caius Marcius. My oath and by armour earlowed.\\n\\nROMEO:\\nWell, my masters? I wish your hand awake: there\\nAs mine own duty, that therefore solet thou the leave?\\n\\nDUCHESS OF YORK:\\nI say in thought our city; in my kingdom star--shout,\\nI had in a kind\\nOf their emmissagion. Richard\\nDefose this brands off one another's son:\\nThis state stir cut a wash it in their\\nhundrewly lips, that can when you partle;\\nThis nothing libiar upon our eating them at the gentlemen\\nBoth are such strength; alone above my seak\\nand it begins their vistal bend: or speak a was;\\nWere he shall bring it; enjoy whose bones\\nIf cheerly, grant thy majesty.\\nCorripth and Cupil, west,\\nThat step an argly age,--af an ishapsory;\\nThese knaves not late the exift of his lips.\\nBut, let it great with Rocation-of an our countrymen:\\nAnd such affecious with a good increportion's glave,\\nIs evil, sighfit\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.439034938812256\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## 導出生成器\n",
        "\n",
        "這個單步模型可以很容易地[保存和恢復]（https://www.tensorflow.org/guide/saved_model），\n",
        "允許您在任何接受 `tf.saved_model` 的地方使用它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d21aa3-0c20-4acf-8658-75c7a2ee8f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f8e77d658d0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c464d3b-61b1-45bb-9085-23427d8f4c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "\n",
            "Groolere:\n",
            "To hid his life\n",
            "Hast thou sure again.\n",
            "\n",
            "BRUTUS:\n",
            "Why heartest thou, sir? first thou repost\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## 高級：定制培訓\n",
        "\n",
        "上面的訓練過程很簡單，但沒有給你太多的控制權。\n",
        "它使用teacher-forcing 來防止錯誤的預測被反饋給模型，因此模型永遠不會學會從錯誤中恢復。\n",
        "\n",
        "現在您已經了解瞭如何手動運行模型，接下來您將實現訓練循環。例如，如果您想實施_課程學習_以幫助穩定模型的開環輸出，這將提供一個起點。\n",
        "\n",
        "自定義訓練循環中最重要的部分是訓練步驟函數。\n",
        "\n",
        "使用 `tf.GradientTape` 來跟踪漸變。您可以通過閱讀 [eager execution guide] (https://www.tensorflow.org/guide/eager) 了解有關此方法的更多信息。\n",
        "\n",
        "基本程序是：\n",
        "\n",
        "1. 執行模型併計算 `tf.GradientTape` 下的損失。\n",
        "2. 計算更新並使用優化器將它們應用於模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "上述 `train_step` 方法的實現遵循 [Keras 的 `train_step` 約定]（https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit）。這是可選的，但它允許您更改訓練步驟的行為並仍然使用 keras 的 `Model.compile` 和 `Model.fit` 方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a9fbdf-9052-4bb1-ad25-01395cb50b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 27s 136ms/step - loss: 2.7142\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e4f1022d0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "\n",
        "或者，如果您需要更多控制，您可以編寫自己的完整自定義訓練循環："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb459985-91e0-4eba-9cb0-85a23aa76366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2124\n",
            "Epoch 1 Batch 50 Loss 2.0632\n",
            "Epoch 1 Batch 100 Loss 1.9641\n",
            "Epoch 1 Batch 150 Loss 1.8704\n",
            "\n",
            "Epoch 1 Loss: 1.9897\n",
            "Time taken for 1 epoch 25.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8679\n",
            "Epoch 2 Batch 50 Loss 1.7662\n",
            "Epoch 2 Batch 100 Loss 1.7425\n",
            "Epoch 2 Batch 150 Loss 1.6463\n",
            "\n",
            "Epoch 2 Loss: 1.7147\n",
            "Time taken for 1 epoch 24.37 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6155\n",
            "Epoch 3 Batch 50 Loss 1.5662\n",
            "Epoch 3 Batch 100 Loss 1.5025\n",
            "Epoch 3 Batch 150 Loss 1.5276\n",
            "\n",
            "Epoch 3 Loss: 1.5550\n",
            "Time taken for 1 epoch 24.46 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4803\n",
            "Epoch 4 Batch 50 Loss 1.5032\n",
            "Epoch 4 Batch 100 Loss 1.4476\n",
            "Epoch 4 Batch 150 Loss 1.4533\n",
            "\n",
            "Epoch 4 Loss: 1.4563\n",
            "Time taken for 1 epoch 24.46 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4028\n",
            "Epoch 5 Batch 50 Loss 1.3743\n",
            "Epoch 5 Batch 100 Loss 1.3894\n",
            "Epoch 5 Batch 150 Loss 1.3860\n",
            "\n",
            "Epoch 5 Loss: 1.3875\n",
            "Time taken for 1 epoch 24.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3237\n",
            "Epoch 6 Batch 50 Loss 1.3181\n",
            "Epoch 6 Batch 100 Loss 1.3181\n",
            "Epoch 6 Batch 150 Loss 1.3534\n",
            "\n",
            "Epoch 6 Loss: 1.3349\n",
            "Time taken for 1 epoch 24.55 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2805\n",
            "Epoch 7 Batch 50 Loss 1.2599\n",
            "Epoch 7 Batch 100 Loss 1.2616\n",
            "Epoch 7 Batch 150 Loss 1.3140\n",
            "\n",
            "Epoch 7 Loss: 1.2902\n",
            "Time taken for 1 epoch 24.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2210\n",
            "Epoch 8 Batch 50 Loss 1.2437\n",
            "Epoch 8 Batch 100 Loss 1.2364\n",
            "Epoch 8 Batch 150 Loss 1.2778\n",
            "\n",
            "Epoch 8 Loss: 1.2494\n",
            "Time taken for 1 epoch 24.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1890\n",
            "Epoch 9 Batch 50 Loss 1.2178\n",
            "Epoch 9 Batch 100 Loss 1.2424\n",
            "Epoch 9 Batch 150 Loss 1.2201\n",
            "\n",
            "Epoch 9 Loss: 1.2099\n",
            "Time taken for 1 epoch 24.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1756\n",
            "Epoch 10 Batch 50 Loss 1.2149\n",
            "Epoch 10 Batch 100 Loss 1.1834\n",
            "Epoch 10 Batch 150 Loss 1.1703\n",
            "\n",
            "Epoch 10 Loss: 1.1714\n",
            "Time taken for 1 epoch 41.09 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2653319/book-example/blob/main/%E9%80%8F%E9%81%8E%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9E%E5%8D%81%E5%9B%9B%E8%A1%8C%E8%A9%A9_%E9%A0%90%E6%B8%AC%E4%B8%8B%E4%B8%80%E5%80%8B%E5%96%AE%E8%A9%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "# 第 4 週：預測下一個單詞\n",
        "\n",
        "歡迎來到這個任務！在這周中，您了解瞭如何創建一個模型來預測文本序列中的下一個單詞，現在您將實現這樣的模型並使用莎士比亞十四行詩語料庫對其進行訓練，同時還創建一些輔助函數來預處理數據。\n",
        "\n",
        "\n",
        "讓我們開始吧！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "對於本作業，您將使用 [莎士比亞十四行詩數據集](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154)，其中包含提取的 2000 多行文本來自莎士比亞的十四行詩。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ4qOUzujMP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4550714d-7c7b-439e-e292-2fe357769d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 49.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfd-nYKij5yY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f954862-d0d0-4805-efa9-efa44993cbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "\n",
        "## 標記文本\n",
        "\n",
        "現在將 Tokenizer 擬合到語料庫並保存單詞總數。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAhM_qAZk0o5"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-0sA46OETa"
      },
      "source": [
        "將文本轉換為序列時，您可以像在本課程中所做的那樣使用 `texts_to_sequences` 方法。\n",
        "\n",
        "在下一個分級功能中，您將需要一次處理該語料庫。鑑於此，重要的是要記住，將數據提供給此方法的方式會影響結果。檢查以下示例以使這一點更清楚。\n",
        "\n",
        "語料庫的第一個示例是一個字符串，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqhPxdeXlfjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2a79a7c3-15f9-41e4-8d66-8f9e44d1443a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFMP4z11O3os"
      },
      "source": [
        "如果您將此文本直接傳遞給 `texts_to_sequences` 方法，您將得到意想不到的結果："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMSEhmbzNZCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f20a053-7f1e-41be-fc49-2568640c5dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " [58],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZmZtpEPEeI"
      },
      "source": [
        "\n",
        "發生這種情況是因為 `texts_to_sequences` 需要一個列表，而您提供的是一個字符串。然而，字符串在 Python 中仍然是可迭代的，因此您將獲得字符串中每個字符的單詞索引。\n",
        "\n",
        "相反，您需要將示例放在列表中，然後再將其傳遞給方法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmgo-vXhk4nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356cc9e2-98b6-4576-9085-09e19b69857b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DU7wK-eQ5dc"
      },
      "source": [
        "\n",
        "請注意，您收到了包含在列表中的序列，因此為了僅獲得所需的序列，您需要顯式獲取列表中的第一項，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpTy8WmIQ57P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8173f3-67f1-44a5-8095-7c42e65fcebf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## 生成 n_grams\n",
        "\n",
        "現在完成下面的 `n_gram_seqs` 函數。此函數接收擬合的標記器和語料庫（這是一個字符串列表），並應返回一個列表，其中包含語料庫中每一行的“n_gram”序列："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy4baJMDl6kj"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: n_gram_seqs\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        "    \"\"\"\n",
        "    生成 n-gram 序列列表\n",
        "    \n",
        "    Args:\n",
        "        corpus (list of string): 用於生成 n-gram 的文本行\n",
        "        tokenizer (object): 包含單詞索引字典的 Tokenizer 類的實例\n",
        "    \n",
        "    Returns:\n",
        "        input_sequences (list of int): 語料庫中每一行的 n-gram 序列\n",
        "    \"\"\"\n",
        "    input_sequences = []\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    for line in corpus:\n",
        "\t    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\t    for i in range(1, len(token_list)):\n",
        "\t\t    n_gram_sequence = token_list[:i+1]\n",
        "\t\t    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlKqW2pfM7G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c8befc-0a56-4d3a-d351-a8e303a7eb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test your function with one example\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HL8Ug6UU0Jt"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for first example look like this:\n",
        "\n",
        "[[34, 417],\n",
        " [34, 417, 877],\n",
        " [34, 417, 877, 166],\n",
        " [34, 417, 877, 166, 213],\n",
        " [34, 417, 877, 166, 213, 517]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtPpCcBjNc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6c5855-287b-4770-d1fb-8463f20564e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 用更大的語料庫測試你的功能\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzecMczU9UB"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for next 3 examples look like this:\n",
        "\n",
        "[[8, 878],\n",
        " [8, 878, 134],\n",
        " [8, 878, 134, 351],\n",
        " [8, 878, 134, 351, 102],\n",
        " [8, 878, 134, 351, 102, 156],\n",
        " [8, 878, 134, 351, 102, 156, 199],\n",
        " [16, 22],\n",
        " [16, 22, 2],\n",
        " [16, 22, 2, 879],\n",
        " [16, 22, 2, 879, 61],\n",
        " [16, 22, 2, 879, 61, 30],\n",
        " [16, 22, 2, 879, 61, 30, 48],\n",
        " [16, 22, 2, 879, 61, 30, 48, 634],\n",
        " [25, 311],\n",
        " [25, 311, 635],\n",
        " [25, 311, 635, 102],\n",
        " [25, 311, 635, 102, 200],\n",
        " [25, 311, 635, 102, 200, 25],\n",
        " [25, 311, 635, 102, 200, 25, 278]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "將 `n_gram_seqs` 轉換應用於整個語料庫並保存最大序列長度以供以後使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laMwiRUpmuSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bccc3d2-e488-4506-9531-10ffcddd3f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OciMdmEdE9L"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_grams of input_sequences have length: 15462\n",
        "maximum length of sequences is: 11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "\n",
        "## 為序列添加填充\n",
        "\n",
        "現在編寫 `pad_seqs` 函數，它將任何給定的序列填充到所需的最大長度。請注意，此函數接收序列列表，並應返回帶有填充序列的 numpy 數組："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "WW1-qAZaWOhC"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: pad_seqs\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "    \"\"\"\n",
        "    將標記化的序列填充到相同的長度\n",
        "    \n",
        "    Args:\n",
        "        input_sequences (list of int): tokenized sequences to pad\n",
        "        maxlen (int): maximum length of the token sequences\n",
        "    \n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sequences padded to the same length\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    padded_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
        "    return padded_sequences\n",
        "    ### END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqVQ0pb3YHLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1508f0-cdc8-4754-ee01-61c73bd8d94f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213],\n",
              "       [417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re_avDznXRnU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "array([[  0,   0,   0,  34, 417],\n",
        "       [  0,   0,  34, 417, 877],\n",
        "       [  0,  34, 417, 877, 166],\n",
        "       [ 34, 417, 877, 166, 213],\n",
        "       [417, 877, 166, 213, 517]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j56_UCOBYzZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d587e785-954b-4d32-8de8-24169f35d54d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmcDluOXcIU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
        "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
        "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
        "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
        "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
        "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
        "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
        "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
        "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
        "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
        "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
        "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
        "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
        "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
        "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
        "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
        "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
        "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
        "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgK-Q_micEYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3eebf6-ae1a-4481-dee2-4fc93fac17c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "# 填充整個語料庫\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59RD1YYNc7CW"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "padded corpus has shape: (15462, 11)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## 將數據拆分為特徵和標籤\n",
        "\n",
        "在將數據輸入神經網絡之前，您應該將其拆分為特徵和標籤。在這種情況下，特徵將是填充的 n_gram 序列，其中最後一個單詞被刪除，標籤將是被刪除的單詞。\n",
        "\n",
        "完成下面的 `features_and_labels` 功能。此函數需要填充的 n_gram 序列作為輸入，並且應該返回一個包含特徵和一個熱編碼標籤的元組。\n",
        "\n",
        "請注意，該函數還接收語料庫中的單詞總數，當對標籤進行熱編碼時，此參數將非常重要，因為語料庫中的每個單詞都至少是一個標籤。如果您需要刷新 `to_categorical` 函數的工作原理，請查看 [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "9WGGbYdnZdmJ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: features_and_labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    \"\"\"\n",
        "    從 n-gram 生成特徵和標籤\n",
        "    \n",
        "    Args:\n",
        "        input_sequences (list of int): sequences to split features and labels from\n",
        "        total_words (int): 詞彙量\n",
        "    \n",
        "    Returns:\n",
        "        features, one_hot_labels (array of int, array of int): 特徵數組和 one-hot 編碼標籤\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23DolaBRaIAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2acae6-f1ff-463a-a35d-2d6c36b82f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34],\n",
              "       [  0,   0,  34, 417],\n",
              "       [  0,  34, 417, 877],\n",
              "       [ 34, 417, 877, 166],\n",
              "       [417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 使用第一個示例的填充 n_grams_seq 測試您的函數\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t4yAx2UaQ43"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "labels have shape: (5, 3211)\n",
        "\n",
        "features look like this:\n",
        "\n",
        "array([[  0,   0,   0,  34],\n",
        "       [  0,   0,  34, 417],\n",
        "       [  0,  34, 417, 877],\n",
        "       [ 34, 417, 877, 166],\n",
        "       [417, 877, 166, 213]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRTuLEt3bRKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53361e54-1e37-4ba3-87c9-9ca9b1d91d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXSMK_HpdLns"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "features have shape: (15462, 10)\n",
        "labels have shape: (15462, 3211)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "##創建模型\n",
        "\n",
        "現在您應該定義一個能夠達到至少 80% 準確度的模型架構。\n",
        "\n",
        "一些提示可以幫助您完成此任務：\n",
        "\n",
        "- 第一層（嵌入）的適當 `output_dim` 為 100，這已經為您提供。\n",
        "- 雙向 LSTM 有助於解決這個特定問題。\n",
        "- 最後一層應該具有與語料庫中單詞總數相同的單元數和一個 softmax 激活函數。\n",
        "- 這個問題可以只用兩層（不包括嵌入）來解決，所以先嘗試小型架構。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "XrE6kpJFfvRY"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "tf.keras.backend.clear_session()\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Creates a text generator model\n",
        "    \n",
        "    Args:\n",
        "        total_words (int): 嵌入層輸入的詞彙表大小\n",
        "        max_sequence_len (int): 輸入序列的長度\n",
        "    \n",
        "    Returns:\n",
        "        model (tf.keras Model): the text generator model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    ### START CODE HERE\n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "    #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1000, return_sequences=True)))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n",
        "    #model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(total_words, activation='softmax'))\n",
        "        \n",
        "\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='sgd',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IpX_Gu_gISk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f8a5b4-6882-43fe-9a99-1f915b6ca001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "484/484 [==============================] - 50s 15ms/step - loss: 6.8354 - accuracy: 0.0224\n",
            "Epoch 2/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 6.4591 - accuracy: 0.0295\n",
            "Epoch 3/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 6.2631 - accuracy: 0.0360\n",
            "Epoch 4/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 6.0485 - accuracy: 0.0398\n",
            "Epoch 5/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 5.7913 - accuracy: 0.0510\n",
            "Epoch 6/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 5.5170 - accuracy: 0.0664\n",
            "Epoch 7/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 5.2506 - accuracy: 0.0768\n",
            "Epoch 8/50\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 4.9798 - accuracy: 0.0936\n",
            "Epoch 9/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 4.6886 - accuracy: 0.1110\n",
            "Epoch 10/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 4.3858 - accuracy: 0.1409\n",
            "Epoch 11/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 4.0446 - accuracy: 0.1733\n",
            "Epoch 12/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 3.7025 - accuracy: 0.2210\n",
            "Epoch 13/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 3.3563 - accuracy: 0.2749\n",
            "Epoch 14/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 3.0062 - accuracy: 0.3309\n",
            "Epoch 15/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 2.6691 - accuracy: 0.3972\n",
            "Epoch 16/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 2.3693 - accuracy: 0.4605\n",
            "Epoch 17/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 2.0949 - accuracy: 0.5172\n",
            "Epoch 18/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.8565 - accuracy: 0.5704\n",
            "Epoch 19/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.6431 - accuracy: 0.6208\n",
            "Epoch 20/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.4673 - accuracy: 0.6580\n",
            "Epoch 21/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.3212 - accuracy: 0.6896\n",
            "Epoch 22/50\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 1.2022 - accuracy: 0.7162\n",
            "Epoch 23/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.0927 - accuracy: 0.7396\n",
            "Epoch 24/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 1.0017 - accuracy: 0.7629\n",
            "Epoch 25/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.9389 - accuracy: 0.7764\n",
            "Epoch 26/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.8744 - accuracy: 0.7921\n",
            "Epoch 27/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.8231 - accuracy: 0.8031\n",
            "Epoch 28/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.7864 - accuracy: 0.8111\n",
            "Epoch 29/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.7583 - accuracy: 0.8151\n",
            "Epoch 30/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.7211 - accuracy: 0.8247\n",
            "Epoch 31/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.7106 - accuracy: 0.8246\n",
            "Epoch 32/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.7016 - accuracy: 0.8253\n",
            "Epoch 33/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6755 - accuracy: 0.8309\n",
            "Epoch 34/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6626 - accuracy: 0.8318\n",
            "Epoch 35/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6424 - accuracy: 0.8348\n",
            "Epoch 36/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6181 - accuracy: 0.8399\n",
            "Epoch 37/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6127 - accuracy: 0.8414\n",
            "Epoch 38/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5999 - accuracy: 0.8451\n",
            "Epoch 39/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5950 - accuracy: 0.8458\n",
            "Epoch 40/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5931 - accuracy: 0.8431\n",
            "Epoch 41/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6136 - accuracy: 0.8372\n",
            "Epoch 42/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.6047 - accuracy: 0.8376\n",
            "Epoch 43/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5723 - accuracy: 0.8465\n",
            "Epoch 44/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5683 - accuracy: 0.8466\n",
            "Epoch 45/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5714 - accuracy: 0.8434\n",
            "Epoch 46/50\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 0.5699 - accuracy: 0.8445\n",
            "Epoch 47/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5748 - accuracy: 0.8409\n",
            "Epoch 48/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5668 - accuracy: 0.8425\n",
            "Epoch 49/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5547 - accuracy: 0.8454\n",
            "Epoch 50/50\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 0.5495 - accuracy: 0.8474\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy72RPgly55q"
      },
      "source": [
        "\n",
        "**要通過此作業，您的模型應達到至少 80% 的訓練準確率**。如果您的模型未達到此閾值，請嘗試使用不同的模型架構再次訓練，考慮增加“LSTM”層中的單元數量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fXTEO3GJ282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "b6538bbe-e493-4079-dd4c-d53725f797da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfaklEQVR4nO3deXhU5d3/8feXIJtikcWigIIVUVzAmoJWat2LS8E+v2rFpe6orVtrtfhzqWjlEcWqKPJIsYi4gE+tlku0iOKC4kIQsIosQcSAImFRRNYk3+ePe1KGmJAhmcmZOfN5Xde5MmfmJPM9MHxyc5/73Le5OyIikvsaRV2AiIikhwJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuWcXMXjSz89J9rEg+MI1Dl/oys3VJuy2ATUB5Yv9Sd3+i4asSyT8KdEkrM/sUuNjdX67mtcbuXtbwVeUW/TlJXanLRTLGzI42s6Vm9kczWw6MMbPdzOx5Mys1szWJxx2Tvuc1M7s48fh8M3vTzIYljl1sZifV8dguZvaGmX1jZi+b2Qgze7yGumursbWZjTGzzxOvP5f0Wn8zm21ma81skZn1TTz/qZkdn3TcrZXvb2adzczN7CIz+wyYmnj+f81suZl9naj9wKTvb25m95jZksTrbyaem2RmV1Y5nw/M7Bc7+vcnuUeBLpnWHmgN7A0MJHzmxiT29wI2AA9u5/t7A/OBtsBdwCNmZnU49kngPaANcCtw7nbes7YaxxG6lg4EdgfuBTCzXsBjwHVAK+Ao4NPtvE9VPwUOAH6W2H8R6Jp4j/eB5K6rYcBhwI8Jf77XAxXAWOCcyoPMrAfQAZi0A3VIrnJ3bdrSthEC7PjE46OBzUCz7RzfE1iTtP8aocsG4HygOOm1FoAD7XfkWEIolwEtkl5/HHg8xXP6T43AHoTg3K2a4x4G7q3tzyWxf2vl+wOdE7Xus50aWiWO+R7hF84GoEc1xzUD1gBdE/vDgIei/lxoa5hNLXTJtFJ331i5Y2YtzOzhRFfBWuANoJWZFdTw/csrH7j7+sTDXXbw2D2B1UnPAZTUVHAtNXZK/Kw11XxrJ2BRTT83Bf+pycwKzOzORLfNWra29NsmtmbVvVfiz3oCcI6ZNQIGEP5HIXlAgS6ZVvWq+7VAN6C3u+9K6JYAqKkbJR2+AFqbWYuk5zpt5/jt1ViS+Fmtqvm+EuAHNfzMbwn/a6jUvppjkv+szgL6A8cTWuWdk2pYCWzcznuNBc4GjgPWu/vbNRwnMaNAl4bWktBd8JWZtQb+lOk3dPclQBFwq5k1MbMjgJ/XpUZ3/4LQt/1Q4uLpTmZWGfiPABeY2XFm1sjMOpjZ/onXZgNnJo4vBH5ZS9ktCcM/VxF+EQxJqqEC+BvwFzPbM9GaP8LMmiZef5vQLXQPap3nFQW6NLT7gOaEVuY7wL8a6H3PBo4gBOSfCd0Sm2o4trYazwW2APOAFcA1AO7+HnAB4SLp18DrhAurADcTWtRrgMGEi7Tb8xiwBFgGzE3UkewPwL+BGcBqYCjb/nt+DDiYcK1A8oTGoUteMrMJwDx3z/j/EKJgZr8GBrp7n6hrkYajFrrkBTP7kZn9INEV0pfQP/1cbd+XixLXCn4DjIq6FmlYCnTJF+0JwxzXAcOBy919VqQVZYCZ/QwoBb6k9m4diRl1uYiIxIRa6CIiMdE4qjdu27atd+7cOaq3FxHJSTNnzlzp7u2qey2yQO/cuTNFRUVRvb2ISE4ysyU1vaYuFxGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiIrJx6CIST+XlsHlz2DZtAjNo0wYaZaj5uHQpfPABVFSEffewATRuDJ06QefO0LJlZt6/Jhs2wMqVYVu1auvjlSvh1FOhsDD976lAF5GUuMO330JJCRQXw8KF23798ssQ4uXl3/3exo2hfXvYc8+tW8eOIdR694Zdd92xWlasgL//HcaPh2nTUvue1q1DsHfuHEK+oiKcT/K2fn345fODH2y7dekCTZuGkP76a/jqq61baWn4pVJSsu22prpFChPat89MoEc2OVdhYaHrTlGR7PHFF/D++zBzJsyZE4Lq66+3bmvXbm0FV2rVCrp2Ddsee0CzZtCkSdiaNg1fKypg+XL4/PNtt9Wrw88wg4MPhh//OGxHHBHCt6xs61bZ6n/rrRDir7wSfm737jBgABx7LOy0U/hZlT/TLPwPoaQEPv10262kJBzfogXsvPPWrUWLcN6LFoXzrWQWfilt2VLzn1+bNuEXRceO4WuHDrD77tC27bbbbruFn1VXZjbT3av9daBAF8kjGzeG1uRnn4Vt0SKYNSuE+PLEEttmsN9+oRW9667wve9t+3XPPUOA77tvCLG6WrsW3nsPpk8P29tvbxuiNdlnnxDiZ54JBx1U9/ffHvfQNbJo0dZt/frwC6xVqxDKlY9btw4h3rx5ZmqpSoEuEmPr14dAfvfdENIbNoTgrtwquwlKSkK3SLJGjeCAA+CHP4TDDgtbjx4N398MoRU+d244j/XrQys2eSsogG7dQo2WySXFs9z2Al196CIR++abEGaV3QRmIWjNtl5YTN42bgzB9847IfzmzNnab92qVWgpNm8euj+aNQuP27SBQw+FvfbaulV2DzRtGu35VyooCF0vBx8cdSW5S4EuEoE1a2DCBBg7NgRzXbRsCb16waBB4cJi796hz1bylwJdpIGUlcHkySHEJ04Mre0DD4TBg0PftHu40Fc57K6iYuvFxarbPvuErpKCgqjPSrKJAl0kA9atgwULYN48mD8/fH3jjXDhsU0bGDgQzjsv9F3nc3+wpFdKgZ5YJf1+oAAY7e53Vnl9L2As0CpxzCB3fyHNtYpkrdJSmDQptLxnzAgjSSo1ahTGMffpA2efDSefHFreIulWa6CbWQEwAjgBWArMMLOJ7j436bCbgKfdfaSZdQdeADpnoF6RrLFgAfzznyHEp08PXSQdO8LRR4fukG7dYP/9w/C+bLnwKPGWSgu9F1Ds7p8AmNl4oD+QHOgOVN7r9T3g83QWKZIt1q2DcePgoYfgww/Dcz17wk03Qf/+YSSJulAkKqkEegegJGl/KdC7yjG3Ai+Z2ZXAzsDxaalOJEsUF8OIETBmTBjTfdhhMHw49OsHe+8ddXUiQbouig4AHnX3e8zsCGCcmR3k7tvcKGxmA4GBAHvttVea3lokM9xhyhS4/3548cUwouT00+HKK+Hww9USl+yTyvxny4BOSfsdE88luwh4GsDd3waaAW2r/iB3H+Xuhe5e2K5du7pVLNIAFi6Ek06Cn/0s3IV5882wZAk8+WSYa0RhLtkolUCfAXQ1sy5m1gQ4E5hY5ZjPgOMAzOwAQqCXprNQkYawYQPcckuYI2T6dLjvvhDkgweHOUxEslmtXS7uXmZmVwCTCUMS/+buH5nZbUCRu08ErgX+ama/I1wgPd+jmiRGpI6efx6uugoWL4azzoJhw8IMgiK5IqU+9MSY8heqPHdL0uO5wJHpLU2kYSxcCNddF4YgHnAATJ0KxxwTdVUiO05L0Ene+uwzuPjiEOJTpsCdd8Ls2QpzyV269V/yzvLlMGQIPPxw2P/tb+GGG8IqMiK5TIEueeOrr2Do0DB+fNMmuOCCMHpFI2glLhTokhcWL4a+fUN/+Vlnwa23hlvyReJEgS6xN3t2GFO+aRO8/jr85CdRVySSGbooKrE2dSocdVRYEPjNNxXmEm8KdImtCRNCN8vee4ebhLp3j7oikcxSoEss3XdfWBX+8MPDwhIdO0ZdkUjmKdAlVioq4Prr4Xe/g1/8Iiz5tttuUVcl0jB0UVRiY8sWuOiiMF/55ZfDAw9ozU3JL2qhSyx8+21YYGLcOLjttjB3ucJc8o1a6JLzVq6EU06BoiIYNQouuSTqikSioUCXnPbpp2HO8s8+g3/8I7TSRfKVAl1y1gcfhGGJGzfCyy/DkZrvU/KcAl1y0syZcNxx0LIlTJsGBx4YdUUi0VOgS86ZMwdOOCEMR3z9dU2uJVJJo1wkp3z4IRx/POyyC7z6qsJcJJkCXXLGvHmhm6VJkzBHS+fOUVckkl0U6JITFi6EY48FsxDmmvpW5LvUhy5Zb/HiEOZbtsBrr0G3blFXJJKdFOiS1T7/PKzxuX59aJlrNItIzRTokrU2bIDTToNVq0LLvEePqCsSyW4KdMlK7jBwIMyYAc89B4cdFnVFItlPF0UlK91zDzz+ONx+u27nF0mVAl2yzgsvhDnNTz8dbrwx6mpEcocCXbLKvHkwYAD07AljxoRhiiKSGgW6ZI01a6BfP2jWLPSb77xz1BWJ5BZdFJWsUFYGv/pVmA5Xt/SL1I0CXbLC4MEwZQqMHq1pcEXqSl0uErnp02HIEDj//LAmqIjUjQJdIvXNN3DuuaGL5f77o65GJLepy0Ui9fvfh7la3ngDdt016mpEcpta6BKZiRNDn/kf/wh9+kRdjUjuU6BLJL78Ei6+OIw3Hzw46mpE4kFdLtLg3EOYr10bhig2aRJ1RSLxoECXBjd6NDz/PNx7r6bDFUkndblIgyouht/9Liwld9VVUVcjEi8KdGlQv/0t7LQTPPooNNKnTySt1OUiDeaNN+Cll2DYMOjYMepqROInpTaSmfU1s/lmVmxmg2o45gwzm2tmH5nZk+ktU3KdO9x8M7RvD5dfHnU1IvFUawvdzAqAEcAJwFJghplNdPe5Scd0BW4AjnT3NWa2e6YKltz0yiuhhf7AA9CiRdTViMRTKi30XkCxu3/i7puB8UDVNWQuAUa4+xoAd1+R3jIll7nDTTdBp05wySVRVyMSX6kEegegJGl/aeK5ZPsB+5nZW2b2jpn1TVeBkvsmTYJ33w1dLk2bRl2NSHyl66JoY6ArcDTQEXjDzA5296+SDzKzgcBAgL004XVeqKiAW26BffYJsymKSOak0kJfBnRK2u+YeC7ZUmCiu29x98XAAkLAb8PdR7l7obsXtmvXrq41Sw559lmYNQtuvTUMVxSRzEkl0GcAXc2si5k1Ac4EJlY55jlC6xwza0vogvkkjXVKDiovhz/9CfbfH846K+pqROKv1i4Xdy8zsyuAyUAB8Dd3/8jMbgOK3H1i4rUTzWwuUA5c5+6rMlm4ZL8JE+Cjj8LXgoKoqxGJP3P3SN64sLDQi4qKInlvybyyMujeHZo3D10uuitUJD3MbKa7F1b3mu4UlYwYNw4WLoTnnlOYizQU/VOTtNuyBW6/HQoLoV+/qKsRyR9qoUvaPfFEWFZu+HAwi7oakfyhFrqkVXk5DBkChx4Kp5wSdTUi+UUtdEmrCRNC3/kzz6h1LtLQ1EKXtKmogDvugIMOgtNOi7oakfyjFrqkzbPPwty58NRTGtkiEgX9s5O0cIc//xn22w9OPz3qakTyk1rokhbPPw+zZ4el5XRXqEg01EKXenMP4867dNGcLSJRUgtd6u2ll2DGDBg1SjMqikRJLXSpl8rWeadOcN55UVcjkt/UQpd6ef11eOstePBBaNIk6mpE8pta6FIvt98O7dvDhRdGXYmIKNClzt59F6ZOhT/8IUyTKyLRUqBLnd19N7RqBZdeGnUlIgIKdKmj4mL4xz/g8sthl12irkZEQIEudfSXv4QhildeGXUlIlJJgS47rLQUxoyBc8+FPfaIuhoRqaRAlx02YgRs3AjXXht1JSKSTIEuO2T9+jDm/Oc/hwMOiLoaEUmmQJcd8uijsGoVXHdd1JWISFUKdElZeXm4GNq7N/TpE3U1IlKVAl1S9uyzsGhRaJ1reTmR7KNAl5S4hxuJ9t1Xy8uJZCtNziUpmTYN3nsPHnpIC1iIZCu10CUld98NbdvC+edHXYmI1ESBLrX6+OOwxNwVV2gSLpFspkCXWj30UJjr/De/iboSEdkeBbps17p18NhjcPrp0K5d1NWIyPYo0GW7nnoK1q4NsyqKSHZToEuN3GHkSDj4YPjxj6OuRkRqo0CXGs2YAbNmhda5biQSyX4KdKnRyJFh8Ypzzom6EhFJhQJdqrV6NYwfH8K8ZcuoqxGRVCjQpVpjx4Y5z3UxVCR3KNDlO9zhf/4nXAg95JCoqxGRVCnQ5TumToUFC+Cyy6KuRER2hAJdvmPkSGjTJtxMJCK5I6VAN7O+ZjbfzIrNbNB2jvt/ZuZmVpi+EqUhff45PPccXHABNGsWdTUisiNqDXQzKwBGACcB3YEBZta9muNaAlcD76a7SGk4o0eHlYkuvTTqSkRkR6XSQu8FFLv7J+6+GRgP9K/muNuBocDGNNYnDaisDEaNghNPDAtZiEhuSSXQOwAlSftLE8/9h5n9EOjk7pO294PMbKCZFZlZUWlp6Q4XK5k1aRIsW6ahiiK5qt4XRc2sEfAX4NrajnX3Ue5e6O6F7TR1X9YZNQr23BNOPTXqSkSkLlIJ9GVAp6T9jonnKrUEDgJeM7NPgcOBibowmltKSuBf/4ILL4TGWphQJCelEugzgK5m1sXMmgBnAhMrX3T3r929rbt3dvfOwDtAP3cvykjFkhGPPgoVFSHQRSQ31Rro7l4GXAFMBj4Gnnb3j8zsNjPrl+kCJfMqKuCRR+D446FLl6irEZG6Suk/1+7+AvBCleduqeHYo+tfljSkV16BJUtg6NCoKxGR+tCdosJf/wqtW8Npp0VdiYjUhwI9z5WWhjtDf/1raNo06mpEpD4U6Hlu3DjYsgUuvjjqSkSkvhToecw93Op/xBFw4IFRVyMi9aURx3ns7bfh44/DCBcRyX1qoeex0aPDmqFnnBF1JSKSDgr0PLV2LUyYAAMGhFAXkdynQM9TTz0F69frYqhInCjQ89To0XDwwfCjH0VdiYikiwI9D82eDUVFoXVuFnU1IpIuCvQ8NHp0uInonHOirkRE0kmBnmdWrYIxY8LIltato65GRNJJgZ5nRo4MF0Ovuy7qSkQk3RToeWTDBhg+HE4+OVwQFZF4UaDnkUcfDZNxXX991JWISCYo0PNEeTkMGwa9e8NRR0VdjYhkguZyyRPPPAOffAJ3362hiiJxpRZ6HnCHu+6C/faD/v2jrkZEMkUt9DwwdSrMnAmjRkFBQdTViEimqIWeB+66C9q3h3PPjboSEckkBXrMzZ4NL70EV18NzZpFXY2IZJICPebuugtatoTLLou6EhHJNAV6jC1eDE8/DZdeCq1aRV2NiGSaAj3G7r0XGjWCa66JuhIRaQgK9JhasSLMqnj22dChQ9TViEhDUKDH1H33wcaNMGhQ1JWISENRoMfQV1/BiBHwy19Ct25RVyMiDUWBHkMPPhgWgb7xxqgrEZGGpECPmXXrwsXQU0+FHj2irkZEGpICPWYefhhWr1brXCQfKdBjZOPGMEXuscfC4YdHXY2INDRNzhUjY8bA8uXwxBNRVyIiUVALPSa2bIGhQ0PL/Jhjoq5GRKKgFnpMPPkkLFkSRrhoAQuR/KQWegyUl8N//3cY1XLKKVFXIyJRUQs9Bp55BubPDxNxqXUukr/UQs9x7jBkSLgj9L/+K+pqRCRKaqHnuGeegTlzYOxYLS8nku/UQs9hW7bADTfAgQeGWRVFJL+lFOhm1tfM5ptZsZl9Z/4+M/u9mc01sw/M7BUz2zv9pUpVo0ZBcXEYrqjWuYjUGuhmVgCMAE4CugMDzKx7lcNmAYXufgjwd+CudBcq2/rmGxg8GH76Uzj55KirEZFskEoLvRdQ7O6fuPtmYDzQP/kAd3/V3dcndt8BOqa3TKlq2DAoLQ1rhmpki4hAaoHeAShJ2l+aeK4mFwEvVveCmQ00syIzKyotLU29StnGF1+EQD/jDOjVK+pqRCRbpPWiqJmdAxQCd1f3uruPcvdCdy9s165dOt86rwweDJs3wx13RF2JiGSTVIYtLgM6Je13TDy3DTM7HrgR+Km7b0pPeVLV/PlhrdDLL4d99426GhHJJqm00GcAXc2si5k1Ac4EJiYfYGaHAg8D/dx9RfrLlEo33ADNm8PNN0ddiYhkm1oD3d3LgCuAycDHwNPu/pGZ3WZm/RKH3Q3sAvyvmc02s4k1/Diph7fegmefhT/+EXbfPepqRCTbmLtH8saFhYVeVFQUyXvnInfo0wcWL4aFC2HnnaOuSESiYGYz3b2wutd063+OeOwxmD49LDGnMBeR6ujW/xzw/vtw2WXhJqILL4y6GhHJVgr0LFdaCr/4BbRrF6bHbaz/U4lIDRQPWaysDH71K/jyS3jzTV0IFZHtU6Bnseuvh1dfDVPjFlZ7CUREZCt1uWSpxx+He++Fq66CX/866mpEJBco0LPQ++/DJZeEi6DDhkVdjYjkCgV6llm5ctuLoDvtFHVFIpIr1IeeRTZuDOuC6iKoiNSFAj1LVFTABRfAtGkwfrwugorIjlOXS5a4+eYQ5HfeGYYqiojsKAV6Fhg9GoYMgYEDw1BFEZG6UKBHbPLkcFt/374wYoSWkxORulOgR2jOHDj9dDjoIN3WLyL1p0CPyLJlcMopsOuuMGkStGwZdUUikusU6BF4/XX4yU9g7Vp44QXosL0lt0VEUqRAb0DffgtXXw1HHw2NGsFLL8Ehh0RdlYjEhQK9gbz5JvTsCcOHw5VXhv7zww+PuioRiRMFeoatXw+//z0cdRSUl4fZE4cP16pDIpJ+GleRQdOmwcUXw4IF8JvfwNChsMsuUVclInGlFnoGfPMNXHFFaJVv3gwvvxzGmCvMRSSTFOhpNnlyGFf+0EPhAuiHH8Jxx0VdlYjkAwV6mqxeDeefH+74bNEiXAS97z71lYtIw1Efej2VlMAjj8DIkbBqFdx4I9x0EzRrFnVlIpJvFOh1UF4OL74IDz8cbgxyhxNPDDMl9uwZdXUikq8U6Dtg9Wp48MEwO2JJCXz/+zBoUBjJ0qVL1NWJSL5ToKegvDyE+I03hm6VE04ICzj366cl4kQkeyjQazF9eriz8/33wzDEBx7Q7foikp00yqUGy5fDeefBkUeGNT6fegpee01hLiLZSy30KlasCCNW7rknLNo8aFDoatFNQSKS7RToCbNmwf33h5b45s2hf/zuu2G//aKuTEQkNXkd6GVl8M9/hiCfNi3cBHTJJaHPvFu3qKsTEdkxeRHoq1fDvHmwcOHWbcECKC6Gdeugc2cYNgwuughatYq6WhGRuolloK9aFVYFevXVcCHzww+3vlZQEAK8a9ewatAxx4TulYKCqKoVEUmPWAT6unUhwKdMCSH+wQfh+RYtwiiVAQPCHZz77htuANLYcRGJo5wM9IoKmD07LOE2eTK89RZs2RLmTznySPjzn8Mybz/6ETRpEnW1IiINI+cCffRouOEGWLky7PfoAddcE+ZS6dNHk2KJSP7KuUDv0CFMUXviieEW/Pbto65IRCQ75Fygn3RS2EREZFsp3fpvZn3NbL6ZFZvZoGpeb2pmExKvv2tmndNdqIiIbF+tgW5mBcAI4CSgOzDAzLpXOewiYI277wvcCwxNd6EiIrJ9qbTQewHF7v6Ju28GxgP9qxzTHxibePx34Dgzs/SVKSIitUkl0DsAJUn7SxPPVXuMu5cBXwNtqv4gMxtoZkVmVlRaWlq3ikVEpFoNOn2uu49y90J3L2zXrl1DvrWISOylEujLgE5J+x0Tz1V7jJk1Br4HrEpHgSIikppUAn0G0NXMuphZE+BMYGKVYyYC5yUe/xKY6u6evjJFRKQ2tY5Dd/cyM7sCmAwUAH9z94/M7DagyN0nAo8A48ysGFhNCH0REWlAFlVD2sxKgSV1/Pa2wMo0lpMr8vW8IX/PXeedX1I5773dvdqLkJEFen2YWZG7F0ZdR0PL1/OG/D13nXd+qe95a5FoEZGYUKCLiMRErgb6qKgLiEi+njfk77nrvPNLvc47J/vQRUTku3K1hS4iIlUo0EVEYiLnAr22udnjwsz+ZmYrzOzDpOdam9kUM1uY+LpblDVmgpl1MrNXzWyumX1kZlcnno/1uZtZMzN7z8zmJM57cOL5Lok1BooTaw7EcpVcMysws1lm9nxiP/bnbWafmtm/zWy2mRUlnqvX5zynAj3Fudnj4lGgb5XnBgGvuHtX4JXEftyUAde6e3fgcOC3ib/juJ/7JuBYd+8B9AT6mtnhhLUF7k2sNbCGsPZAHF0NfJy0ny/nfYy790wae16vz3lOBTqpzc0eC+7+BmEahWTJ886PBU5r0KIagLt/4e7vJx5/Q/hH3oGYn7sH6xK7OyU2B44lrDEAMTxvADPrCJwCjE7sG3lw3jWo1+c81wI9lbnZ4+z77v5F4vFy4PtRFpNpiaUMDwXeJQ/OPdHtMBtYAUwBFgFfJdYYgPh+3u8DrgcqEvttyI/zduAlM5tpZgMTz9Xrc55zi0RL4O5uZrEdc2pmuwDPANe4+9rkBbDieu7uXg70NLNWwLPA/hGXlHFmdiqwwt1nmtnRUdfTwPq4+zIz2x2YYmbzkl+sy+c811roqczNHmdfmtkeAImvKyKuJyPMbCdCmD/h7v9IPJ0X5w7g7l8BrwJHAK0SawxAPD/vRwL9zOxTQhfqscD9xP+8cfdlia8rCL/Ae1HPz3muBXoqc7PHWfK88+cB/4ywloxI9J8+Anzs7n9JeinW525m7RItc8ysOXAC4frBq4Q1BiCG5+3uN7h7R3fvTPj3PNXdzybm521mO5tZy8rHwInAh9Tzc55zd4qa2cmEPrfKudnviLikjDCzp4CjCdNpfgn8CXgOeBrYizD18BnuXvXCaU4zsz7ANODfbO1T/f+EfvTYnruZHUK4CFZAaGg97e63mdk+hJZra2AWcI67b4qu0sxJdLn8wd1Pjft5J87v2cRuY+BJd7/DzNpQj895zgW6iIhUL9e6XEREpAYKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITPwfE4UYxFSuAuMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 查看模型的訓練曲線\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvED5A3qrn2"
      },
      "source": [
        "在結束作業之前，請務必下載 `history.pkl` 文件，其中包含您模型的訓練歷史信息，並將用於計算您的成績。您可以通過運行以下單元格來下載此文件："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QRG73l6qE-c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d0409250-5951-48c2-9d59-f68d110e4269"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_add77b34-2516-4065-9607-f9b223d064da\", \"history.pkl\", 944)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "\n",
        "## 看看你的模型在行動\n",
        "\n",
        "完成所有工作後，終於到了查看模型生成文本的時候了。\n",
        "\n",
        "運行下面的單元格以生成種子文本的下 100 個單詞。\n",
        "\n",
        "提交作業後，我們鼓勵您嘗試針對不同數量的 epoch 進行訓練，並了解這如何影響生成文本的連貫性。還可以嘗試更改種子文本，看看你會得到什麼！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870d46b4-4fb4-49ef-b8ae-37e6f6ebd416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope of this place alone i that i live more dearly fall every worth is best is fall on me erred erred groan merits not new survey survey ' told thee love is not 'no ' slain slain slain bearing thee love alone all thy side kill'd halt halt fall so ill ' ' ' thy heart that grows doth heart doth oppress'd with staineth known injury of shade ' new ' ill be is due of thee that is thy 'will ' die fair that due so hate be so deem'd well heart can live begin live more lies more\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\t# Convert the text into sequences\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t# Pad the sequences\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t# Get the probabilities of predicting a word\n",
        "\tpredicted = model.predict(token_list, verbose=0)\n",
        "\t# Choose the next word based on the maximum probability\n",
        "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
        "\t# Get the actual word from the word index\n",
        "\toutput_word = tokenizer.index_word[predicted]\n",
        "\t# Append to the current text\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQVDbdcYqSux"
      },
      "source": [
        "\n",
        "您還需要提交此筆記本進行評分。要下載它，請單擊屏幕左上角的“文件”選項卡，然後單擊“下載”->“下載 .ipynb”。只要它是有效的 .ipynb (jupyter notebook) 文件，您就可以將其命名為任何您想要的名稱。\n",
        "\n",
        "**恭喜你完成了本週的作業！**\n",
        "\n",
        "您已經成功實現了一個能夠預測文本序列中下一個單詞的神經網絡！\n",
        "\n",
        "**我們希望在下一個專業課程中見到您！保持！**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}